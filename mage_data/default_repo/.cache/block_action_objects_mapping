{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/export_invoices_raw.py:data_exporter:python:export invoices raw": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\nimport psycopg2\nimport json\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    if data is None or data.empty:\n        log(\"WARN\", \"No llegaron datos para exportar. Pipeline finalizado.\")\n        return\n\n    try:\n        db_host = get_secret_value('POSTGREST_HOST')\n        db_name = get_secret_value('POSTGRES_DB')\n        db_user = get_secret_value('POSTGRES_USER')\n        db_pass = get_secret_value('POSTGRES_PASSWORD')\n    except Exception as e:\n        log(\"CONFIG_ERROR\", f\"Error en credenciales, {e}\")\n        return\n\n    log(\"DATA_PROCESS\", f\"Guardando {len(data)} datos\")\n\n    conn = psycopg2.connect(\n        host=db_host,\n        database=db_name,\n        user=db_user,\n        password=db_pass\n    )\n    cursor = conn.cursor()\n\n    insert_query = \"\"\"\n    INSERT INTO raw.qb_invoices_backfill (\n        id, payload, ingested_at_utc, extract_window_start_utc, \n        extract_window_end_utc, page_number, page_size, request_payload\n    ) \n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (id) DO UPDATE SET\n        payload = EXCLUDED.payload,\n        ingested_at_utc = EXCLUDED.ingested_at_utc,\n        extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n        extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n        page_number = EXCLUDED.page_number,\n        page_size = EXCLUDED.page_size,\n        request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    success_count = 0\n    error_count = 0\n\n    try:\n        for _, row in data.iterrows():\n\n            try:\n                cursor.execute(insert_query, (\n                    str(row['id']),\n                    json.dumps(row['payload']),\n                    row['ingested_at_utc'],\n                    row['extract_window_start_utc'],\n                    row['extract_window_end_utc'],\n                    row['page_number'],\n                    row['page_size'],\n                    json.dumps(row['request_payload'])\n                ))\n                success_count += 1\n            except Exception as e:\n                error_count += 1\n                log(\"ROW_ERROR\", f\"Fallo en ID {row.get('id', 'unknown')}: {e}\")\n                conn.rollback()\n                raise e\n\n        \n        conn.commit()\n        log(\"METRIC\", \"Datos guardados correctamente\")\n    except Exception as e:\n        conn.rollback()\n        log(\"CRITICAL\", f\"Error al guardar datos: {e}\")\n        raise e\n    finally:\n        cursor.close()\n        conn.close()", "file_path": "data_exporters/export_invoices_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_invoices_raw"}, "data_exporters/export_customers_raw.py:data_exporter:python:export customers raw": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    if data is None or data.empty:\n        log(\"WARN\", \"No llegaron datos para exportar. Pipeline finalizado.\")\n        return\n\n    try:\n        db_host = get_secret_value('POSTGREST_HOST')\n        db_name = get_secret_value('POSTGRES_DB')\n        db_user = get_secret_value('POSTGRES_USER')\n        db_pass = get_secret_value('POSTGRES_PASSWORD')\n    except Exception as e:\n        log(\"CONFIG_ERROR\", f\"Error en credenciales, {e}\")\n        return\n\n    log(\"DATA_PROCESS\", f\"Guardando {len(data)} datos\")\n\n    conn = psycopg2.connect(\n        host=db_host,\n        database=db_name,\n        user=db_user,\n        password=db_pass\n    )\n    cursor = conn.cursor()\n\n    insert_query = \"\"\"\n    INSERT INTO raw.qb_customers_backfill (\n        id, payload, ingested_at_utc, extract_window_start_utc, \n        extract_window_end_utc, page_number, page_size, request_payload\n    ) \n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (id) DO UPDATE SET\n        payload = EXCLUDED.payload,\n        ingested_at_utc = EXCLUDED.ingested_at_utc,\n        extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n        extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n        page_number = EXCLUDED.page_number,\n        page_size = EXCLUDED.page_size,\n        request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    success_count = 0\n    error_count = 0\n\n    try:\n        for _, row in data.iterrows():\n\n            try:\n                cursor.execute(insert_query, (\n                    str(row['id']),\n                    json.dumps(row['payload']),\n                    row['ingested_at_utc'],\n                    row['extract_window_start_utc'],\n                    row['extract_window_end_utc'],\n                    row['page_number'],\n                    row['page_size'],\n                    json.dump(row['request_payload'])\n                ))\n                success_count += 1\n            except Exception as e:\n                error_count += 1\n                log(\"ROW_ERROR\", f\"Fallo en ID {row.get('id', 'unknown')}: {row_error}\")\n                conn.rollback()\n                raise row_error\n\n        \n        conn.commit()\n        log(\"METRIC\", \"Datos guardados correctamente\")\n    except Exception as e:\n        conn.rollback()\n        log(\"CRITICAL\", f\"Error al guardar datos: {e}\")\n        raise e\n    finally:\n        cursor.close()\n        conn.close()", "file_path": "data_exporters/export_customers_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_customers_raw"}, "data_exporters/export_items_raw.py:data_exporter:python:export items raw": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nimport psycopg2\nimport json\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    if data is None or data.empty:\n        log(\"WARN\", \"No llegaron datos para exportar. Pipeline finalizado.\")\n        return\n\n    try:\n        db_host = get_secret_value('POSTGREST_HOST')\n        db_name = get_secret_value('POSTGRES_DB')\n        db_user = get_secret_value('POSTGRES_USER')\n        db_pass = get_secret_value('POSTGRES_PASSWORD')\n    except Exception as e:\n        log(\"CONFIG_ERROR\", f\"Error en credenciales, {e}\")\n        return\n\n    log(\"DATA_PROCESS\", f\"Guardando {len(data)} datos\")\n\n    conn = psycopg2.connect(\n        host=db_host,\n        database=db_name,\n        user=db_user,\n        password=db_pass\n    )\n    cursor = conn.cursor()\n\n    insert_query = \"\"\"\n    INSERT INTO raw.qb_invoices_backfill (\n        id, payload, ingested_at_utc, extract_window_start_utc, \n        extract_window_end_utc, page_number, page_size, request_payload\n    ) \n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (id) DO UPDATE SET\n        payload = EXCLUDED.payload,\n        ingested_at_utc = EXCLUDED.ingested_at_utc,\n        extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n        extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n        page_number = EXCLUDED.page_number,\n        page_size = EXCLUDED.page_size,\n        request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    success_count = 0\n    error_count = 0\n\n    try:\n        for _, row in data.iterrows():\n\n            try:\n                cursor.execute(insert_query, (\n                    str(row['id']),\n                    json.dumps(row['payload']),\n                    row['ingested_at_utc'],\n                    row['extract_window_start_utc'],\n                    row['extract_window_end_utc'],\n                    row['page_number'],\n                    row['page_size'],\n                    json.dump(row['request_payload'])\n                ))\n                success_count += 1\n            except Exception as e:\n                error_count += 1\n                log(\"ROW_ERROR\", f\"Fallo en ID {row.get('id', 'unknown')}: {row_error}\")\n                conn.rollback()\n                raise row_error\n\n        \n        conn.commit()\n        log(\"METRIC\", \"Datos guardados correctamente\")\n    except Exception as e:\n        conn.rollback()\n        log(\"CRITICAL\", f\"Error al guardar datos: {e}\")\n        raise e\n    finally:\n        cursor.close()\n        conn.close()", "file_path": "data_exporters/export_items_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_items_raw"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/load_invoices_qbo.py:data_loader:python:load invoices qbo": {"content": "import io\nimport pandas as pd\nimport requests\nimport base64\nimport time\nfrom datetime import timedelta, datetime\nimport pytz\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\ndef get_access_token():\n    log(\"AUTH\", \"Solicitando nuevo token...\")\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_str = f'{client_id}:{client_secret}'\n    b64_auth = base64.b64encode(auth_str.encode()).decode()\n\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    headers = {\n        'Authorization' : f'Basic {b64_auth}',\n        'Content-Type' : 'application/x-www-form-urlencoded',\n        'Accept' : 'application/json'\n    }\n\n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data)\n        if response.status_code != 200:\n            log(\"AUTH_ERROR\", f\"Fallo al renovar: {response.text}\")\n            raise Exception(f\"Error while renewing token: {response.text}\")\n        return response.json().get('access_token')\n    except Exception as e:\n        log(\"AUTH_ERROR\", str(e))\n        raise e\n    \n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    now = datetime.now(pytz.utc)\n    default_start = (now - timedelta(days=2)).strftime('%Y-%m-%d')\n    default_end = now.strftime('%Y-%m-%d')\n\n    start_date_str = kwargs.get('fecha_inicio', default_start)\n    end_date_str = kwargs.get('fecha_fin', default_end)\n\n    log(\"INIT\", f\"Iniciando extracci\u00f3n desde {start_date_str} hasta {end_date_str}\")\n\n    access_token = get_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n\n    #query = \"SELECT * FROM Customer STARTPOSITION 1 MAXRESULTS 1\"\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion=65\"\n\n    headers = {\n        'Authorization' : f'Bearer {access_token}',\n        'Accept' : 'application/json',\n        'Content-Type' : 'application/json'\n    }\n\n    all_records = []\n\n    start_dt = pd.to_datetime(start_date_str).replace(tzinfo=pytz.UTC)\n    end_dt = pd.to_datetime(end_date_str).replace(tzinfo=pytz.UTC)\n\n    print(f\"Rango de datos: {start_dt} hasta {end_dt}\")\n\n    current_date = start_dt\n\n    while current_date <= end_dt:\n\n        chunk_start_time = time.time()\n        daily_pages = 0\n        daily_rows = 0\n\n        next_date = current_date + timedelta(days=1)\n\n        start_fmt = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        end_fmt = next_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        start_position = 1\n        max_results = 1000\n\n        while True:\n            query = (\n                f\"SELECT * FROM Invoice \"\n                f\"WHERE MetaData.LastUpdatedTime >= '{start_fmt}' \"\n                f\"AND MetaData.LastUpdatedTime <= '{end_fmt}' \"\n                f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n            )\n\n            max_retries = 5\n            success = False\n\n            for attempt in range(max_retries):\n                try:\n                    response = requests.get(base_url, headers=headers, params={'query':query})\n                    if response.status_code == 401:\n                        log(\"AUTH\", 'Token expirado, obteniendo uno nuevo...')\n                        access_token = get_access_token()\n                        headers['Authorization'] = f\"Bearer {access_token}\"\n                        continue\n\n                    if response.status_code == 429:\n                        wait_time = 2 ** attempt \n                        log(\"WARN\", f\"Rate Limit (429). Esperando {wait_time}s antes del reintento {attempt+1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                        continue\n\n                    if response.status_code >= 500:\n                        wait_time = 2 ** attempt\n                        log(\"SERVER\", f\"Error Servidor ({response.status_code}). Esperando {wait_time}s...\")\n                        time.sleep(wait_time)\n                        continue\n                    \n                    if response.status_code != 200:\n                        log(\"ERROR\", f\"Error al obtener datos del dia: {current_date}: {response.text}\")\n                        break\n                    \n                    success = True\n                    break\n\n                except Exception as e:\n                    log(\"ERROR\", \"Error de conexion: {e}\")\n                    time.sleep(2 ** attempt)\n                \n            if not success:\n                log(\"ERROR\", \"Se extendieron los intentos para el dia: {current_date.date()}\")\n                break\n\n            data = response.json()\n            batch = data.get('QueryResponse', {}).get('Invoice', [])\n\n            if not batch:\n                break\n\n            daily_pages += 1\n            daily_rows += len(batch)\n\n            ingestion_time = datetime.now(pytz.utc)\n\n            for item in batch:\n                record = {\n                    'id': item['Id'], \n                    'payload': item,  \n                    'ingested_at_utc': ingestion_time,\n                    'extract_window_start_utc': start_fmt,\n                    'extract_window_end_utc': end_fmt,\n                    'page_number': (start_position // max_results) + 1,\n                    'page_size': len(batch),\n                    'request_payload': query \n                }\n                all_records.append(record)\n\n            start_position += max_results\n            if len(batch) < max_results:\n                break\n                \n\n        chunk_duration = time.time() - chunk_start_time\n        log(\"METRIC\", f\"D\u00eda: {current_date.date()} | Filas: {daily_rows} | Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        if daily_rows > 0:\n            log(\"REPORTE\", f\"Reporte tramo {current_date.date()}:\")\n            log(\"REPORTE\", f\"Paginas: {daily_pages}\")\n            log(\"REPORTE\", f\"Filas: {daily_rows}\")\n            log(\"REPORTE\", f\"Duracion: {chunk_duration:.2f}\")\n        else:\n            log(\"VOLUMETRY_WARN\", f\"0 registros para {current_date.date()}\")\n\n        current_date = next_date\n    \n    log(\"DONE\", f'Total: {len(all_records)} registros')\n    return pd.DataFrame(all_records)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_invoices_qbo.py", "language": "python", "type": "data_loader", "uuid": "load_invoices_qbo"}, "data_loaders/load_qdo_data.py:data_loader:python:load qdo data": {"content": "import io\nimport pandas as pd\nimport requests\nimport base64\nimport time\nfrom datetime import timedelta, datetime\nimport pytz\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\ndef get_access_token():\n    log(\"AUTH\", \"Solicitando nuevo token...\")\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_str = f'{client_id}:{client_secret}'\n    b64_auth = base64.b64encode(auth_str.encode()).decode()\n\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    headers = {\n        'Authorization' : f'Basic {b64_auth}',\n        'Content-Type' : 'application/x-www-form-urlencoded',\n        'Accept' : 'application/json'\n    }\n\n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data)\n        if response.status_code != 200:\n            log(\"AUTH_ERROR\", f\"Fallo al renovar: {response.text}\")\n            raise Exception(f\"Error while renewing token: {response.text}\")\n        return response.json().get('access_token')\n    except Exception as e:\n        log(\"AUTH_ERROR\", str(e))\n        raise e\n    \n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    now = datetime.now(pytz.utc)\n    default_start = (now - timedelta(days=2)).strftime('%Y-%m-%d')\n    default_end = now.strftime('%Y-%m-%d')\n\n    start_date_str = kwargs.get('fecha_inicio', default_start)\n    end_date_str = kwargs.get('fecha_fin', default_end)\n\n    log(\"INIT\", f\"Iniciando extracci\u00f3n desde {start_date_str} hasta {end_date_str}\")\n\n    access_token = get_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n\n    #query = \"SELECT * FROM Customer STARTPOSITION 1 MAXRESULTS 1\"\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion=65\"\n\n    headers = {\n        'Authorization' : f'Bearer {access_token}',\n        'Accept' : 'application/json',\n        'Content-Type' : 'application/json'\n    }\n\n    all_records = []\n\n    start_dt = pd.to_datetime(start_date_str).replace(tzinfo=pytz.UTC)\n    end_dt = pd.to_datetime(end_date_str).replace(tzinfo=pytz.UTC)\n\n    print(f\"Rango de datos: {start_dt} hasta {end_dt}\")\n\n    current_date = start_dt\n\n    while current_date <= end_dt:\n\n        chunk_start_time = time.time()\n        daily_pages = 0\n        daily_rows = 0\n\n        next_date = current_date + timedelta(days=1)\n\n        start_fmt = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        end_fmt = next_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        start_position = 1\n        max_results = 1000\n\n        while True:\n            query = (\n                f\"SELECT * FROM Customer \"\n                f\"WHERE MetaData.LastUpdatedTime >= '{start_fmt}' \"\n                f\"AND MetaData.LastUpdatedTime <= '{end_fmt}' \"\n                f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n            )\n\n            max_retries = 5\n            success = False\n\n            for attempt in range(max_retries):\n                try:\n                    response = requests.get(base_url, headers=headers, params={'query':query})\n                    if response.status_code == 401:\n                        log(\"AUTH\", 'Token expirado, obteniendo uno nuevo...')\n                        access_token = get_access_token()\n                        headers['Authorization'] = f\"Bearer {access_token}\"\n                        continue\n\n                    if response.status_code == 429:\n                        wait_time = 2 ** attempt \n                        log(\"WARN\", f\"Rate Limit (429). Esperando {wait_time}s antes del reintento {attempt+1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                        continue\n\n                    if response.status_code >= 500:\n                        wait_time = 2 ** attempt\n                        log(\"SERVER\", f\"Error Servidor ({response.status_code}). Esperando {wait_time}s...\")\n                        time.sleep(wait_time)\n                        continue\n                    \n                    if response.status_code != 200:\n                        log(\"ERROR\", f\"Error al obtener datos del dia: {current_date}: {response.text}\")\n                        break\n                    \n                    success = True\n                    break\n\n                except Exception as e:\n                    log(\"ERROR\", \"Error de conexion: {e}\")\n                    time.sleep(2 ** attempt)\n                \n            if not success:\n                log(\"ERROR\", \"Se extendieron los intentos para el dia: {current_date.date()}\")\n                break\n\n            data = response.json()\n            batch = data.get('QueryResponse', {}).get('Customer', [])\n\n            if not batch:\n                break\n\n            daily_pages += 1\n            daily_rows += len(batch)\n\n            ingestion_time = datetime.now(pytz.utc)\n\n            for item in batch:\n                record = {\n                    'id': item['Id'], \n                    'payload': item,  \n                    'ingested_at_utc': ingestion_time,\n                    'extract_window_start_utc': start_fmt,\n                    'extract_window_end_utc': end_fmt,\n                    'page_number': (start_position // max_results) + 1,\n                    'page_size': len(batch),\n                    'request_payload': query \n                }\n                all_records.append(record)\n\n            start_position += max_results\n            if len(batch) < max_results:\n                break\n                \n\n        chunk_duration = time.time() - chunk_start_time\n        log(\"METRIC\", f\"D\u00eda: {current_date.date()} | Filas: {daily_rows} | Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        if daily_rows > 0:\n            log(\"REPORTE\", f\"Reporte tramo {current_data.date()}:\")\n            log(\"REPORTE\", f\"Paginas: {daily_pages}\")\n            log(\"REPORTE\", f\"Filas: {daily_rows}\")\n            log(\"REPORTE\", f\"Duracion: {chunk_duration:.2f}\")\n        else:\n            log(\"VOLUMETRY_WARN\", f\"0 registros para {current_date.date()}\")\n\n        current_date = next_date\n    \n    log(\"DONE\", f'Total: {len(all_records)} registros')\n    return pd.DataFrame(all_records)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_qdo_data.py", "language": "python", "type": "data_loader", "uuid": "load_qdo_data"}, "data_loaders/load_items_qbo.py:data_loader:python:load items qbo": {"content": "import io\nimport pandas as pd\nimport requests\nimport base64\nimport time\nfrom datetime import timedelta, datetime\nimport pytz\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\ndef get_access_token():\n    log(\"AUTH\", \"Solicitando nuevo token...\")\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_str = f'{client_id}:{client_secret}'\n    b64_auth = base64.b64encode(auth_str.encode()).decode()\n\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    headers = {\n        'Authorization' : f'Basic {b64_auth}',\n        'Content-Type' : 'application/x-www-form-urlencoded',\n        'Accept' : 'application/json'\n    }\n\n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data)\n        if response.status_code != 200:\n            log(\"AUTH_ERROR\", f\"Fallo al renovar: {response.text}\")\n            raise Exception(f\"Error while renewing token: {response.text}\")\n        return response.json().get('access_token')\n    except Exception as e:\n        log(\"AUTH_ERROR\", str(e))\n        raise e\n    \n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    now = datetime.now(pytz.utc)\n    default_start = (now - timedelta(days=2)).strftime('%Y-%m-%d')\n    default_end = now.strftime('%Y-%m-%d')\n\n    start_date_str = kwargs.get('fecha_inicio', default_start)\n    end_date_str = kwargs.get('fecha_fin', default_end)\n\n    log(\"INIT\", f\"Iniciando extracci\u00f3n desde {start_date_str} hasta {end_date_str}\")\n\n    access_token = get_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n\n    #query = \"SELECT * FROM Customer STARTPOSITION 1 MAXRESULTS 1\"\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion=65\"\n\n    headers = {\n        'Authorization' : f'Bearer {access_token}',\n        'Accept' : 'application/json',\n        'Content-Type' : 'application/json'\n    }\n\n    all_records = []\n\n    start_dt = pd.to_datetime(start_date_str).replace(tzinfo=pytz.UTC)\n    end_dt = pd.to_datetime(end_date_str).replace(tzinfo=pytz.UTC)\n\n    print(f\"Rango de datos: {start_dt} hasta {end_dt}\")\n\n    current_date = start_dt\n\n    while current_date <= end_dt:\n\n        chunk_start_time = time.time()\n        daily_pages = 0\n        daily_rows = 0\n\n        next_date = current_date + timedelta(days=1)\n\n        start_fmt = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        end_fmt = next_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        start_position = 1\n        max_results = 1000\n\n        while True:\n            query = (\n                f\"SELECT * FROM Invoice \"\n                f\"WHERE MetaData.LastUpdatedTime >= '{start_fmt}' \"\n                f\"AND MetaData.LastUpdatedTime <= '{end_fmt}' \"\n                f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n            )\n\n            max_retries = 5\n            success = False\n\n            for attempt in range(max_retries):\n                try:\n                    response = requests.get(base_url, headers=headers, params={'query':query})\n                    if response.status_code == 401:\n                        log(\"AUTH\", 'Token expirado, obteniendo uno nuevo...')\n                        access_token = get_access_token()\n                        headers['Authorization'] = f\"Bearer {access_token}\"\n                        continue\n\n                    if response.status_code == 429:\n                        wait_time = 2 ** attempt \n                        log(\"WARN\", f\"Rate Limit (429). Esperando {wait_time}s antes del reintento {attempt+1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                        continue\n\n                    if response.status_code >= 500:\n                        wait_time = 2 ** attempt\n                        log(\"SERVER\", f\"Error Servidor ({response.status_code}). Esperando {wait_time}s...\")\n                        time.sleep(wait_time)\n                        continue\n                    \n                    if response.status_code != 200:\n                        log(\"ERROR\", f\"Error al obtener datos del dia: {current_date}: {response.text}\")\n                        break\n                    \n                    success = True\n                    break\n\n                except Exception as e:\n                    log(\"ERROR\", \"Error de conexion: {e}\")\n                    time.sleep(2 ** attempt)\n                \n            if not success:\n                log(\"ERROR\", \"Se extendieron los intentos para el dia: {current_date.date()}\")\n                break\n\n            data = response.json()\n            batch = data.get('QueryResponse', {}).get('Invoice', [])\n\n            if not batch:\n                break\n\n            daily_pages += 1\n            daily_rows += len(batch)\n\n            ingestion_time = datetime.now(pytz.utc)\n\n            for item in batch:\n                record = {\n                    'id': item['Id'], \n                    'payload': item,  \n                    'ingested_at_utc': ingestion_time,\n                    'extract_window_start_utc': start_fmt,\n                    'extract_window_end_utc': end_fmt,\n                    'page_number': (start_position // max_results) + 1,\n                    'page_size': len(batch),\n                    'request_payload': query \n                }\n                all_records.append(record)\n\n            start_position += max_results\n            if len(batch) < max_results:\n                break\n                \n\n        chunk_duration = time.time() - chunk_start_time\n        log(\"METRIC\", f\"D\u00eda: {current_date.date()} | Filas: {daily_rows} | Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        if daily_rows > 0:\n            log(\"REPORTE\", f\"Reporte tramo {current_data.date()}:\")\n            log(\"REPORTE\", f\"Paginas: {daily_pages}\")\n            log(\"REPORTE\", f\"Filas: {daily_rows}\")\n            log(\"REPORTE\", f\"Duracion: {chunk_duration:.2f}\")\n        else:\n            log(\"VOLUMETRY_WARN\", f\"0 registros para {current_date.date()}\")\n\n        current_date = next_date\n    \n    log(\"DONE\", f'Total: {len(all_records)} registros')\n    return pd.DataFrame(all_records)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_items_qbo.py", "language": "python", "type": "data_loader", "uuid": "load_items_qbo"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_customers_raw\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_qdo_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_qdo_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_customers_raw\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_qdo_data\n  uuid: export_customers_raw\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-24 14:38:55.900205+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  fecha_fin: '2026-01-24'\n  fecha_inicio: '2015-01-01'\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/load_invoices_qbo.py\n    file_source:\n      path: data_loaders/load_invoices_qbo.py\n  downstream_blocks:\n  - export_invoices_raw\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_invoices_qbo\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_invoices_qbo\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/export_invoices_raw.py\n    file_source:\n      path: data_exporters/export_invoices_raw.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_invoices_raw\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_invoices_qbo\n  uuid: export_invoices_raw\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-26 23:45:10.384529+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_items_raw\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_items_qbo\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_items_qbo\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_items_raw\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_items_qbo\n  uuid: export_items_raw\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-26 23:49:03.925462+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}, "/home/src/default_repo/data_loaders/load_qdo_data.py:data_loader:python:home/src/default repo/data loaders/load qdo data": {"content": "import io\nimport pandas as pd\nimport requests\nimport base64\nimport time\nfrom datetime import timedelta, datetime\nimport pytz\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\ndef get_access_token():\n    log(\"AUTH\", \"Solicitando nuevo token...\")\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_str = f'{client_id}:{client_secret}'\n    b64_auth = base64.b64encode(auth_str.encode()).decode()\n\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    headers = {\n        'Authorization' : f'Basic {b64_auth}',\n        'Content-Type' : 'application/x-www-form-urlencoded',\n        'Accept' : 'application/json'\n    }\n\n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data)\n        if response.status_code != 200:\n            log(\"AUTH_ERROR\", f\"Fallo al renovar: {response.text}\")\n            raise Exception(f\"Error while renewing token: {response.text}\")\n        return response.json().get('access_token')\n    except Exception as e:\n        log(\"AUTH_ERROR\", str(e))\n        raise e\n    \n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    now = datetime.now(pytz.utc)\n    default_start = (now - timedelta(days=2)).strftime('%Y-%m-%d')\n    default_end = now.strftime('%Y-%m-%d')\n\n    start_date_str = kwargs.get('fecha_inicio', default_start)\n    end_date_str = kwargs.get('fecha_fin', default_end)\n\n    log(\"INIT\", f\"Iniciando extracci\u00f3n desde {start_date_str} hasta {end_date_str}\")\n\n    access_token = get_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n\n    #query = \"SELECT * FROM Customer STARTPOSITION 1 MAXRESULTS 1\"\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion=65\"\n\n    headers = {\n        'Authorization' : f'Bearer {access_token}',\n        'Accept' : 'application/json',\n        'Content-Type' : 'application/json'\n    }\n\n    all_records = []\n\n    start_dt = pd.to_datetime(start_date_str).replace(tzinfo=pytz.UTC)\n    end_dt = pd.to_datetime(end_date_str).replace(tzinfo=pytz.UTC)\n\n    print(f\"Rango de datos: {start_dt} hasta {end_dt}\")\n\n    current_date = start_dt\n\n    while current_date <= end_dt:\n\n        chunk_start_time = time.time()\n        daily_pages = 0\n        daily_rows = 0\n\n        next_date = current_date + timedelta(days=1)\n\n        start_fmt = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        end_fmt = next_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        start_position = 1\n        max_results = 1000\n\n        while True:\n            query = (\n                f\"SELECT * FROM Customer \"\n                f\"WHERE MetaData.LastUpdatedTime >= '{start_fmt}' \"\n                f\"AND MetaData.LastUpdatedTime <= '{end_fmt}' \"\n                f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n            )\n\n            max_retries = 5\n            success = False\n\n            for attempt in range(max_retries):\n                try:\n                    response = requests.get(base_url, headers=headers, params={'query':query})\n                    if response.status_code == 401:\n                        log(\"AUTH\", 'Token expirado, obteniendo uno nuevo...')\n                        access_token = get_access_token()\n                        headers['Authorization'] = f\"Bearer {access_token}\"\n                        continue\n\n                    if response.status_code == 429:\n                        wait_time = 2 ** attempt \n                        log(\"WARN\", f\"Rate Limit (429). Esperando {wait_time}s antes del reintento {attempt+1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                        continue\n\n                    if response.status_code >= 500:\n                        wait_time = 2 ** attempt\n                        log(\"SERVER\", f\"Error Servidor ({response.status_code}). Esperando {wait_time}s...\")\n                        time.sleep(wait_time)\n                        continue\n                    \n                    if response.status_code != 200:\n                        log(\"ERROR\", f\"Error al obtener datos del dia: {current_date}: {response.text}\")\n                        break\n                    \n                    success = True\n                    break\n\n                except Exception as e:\n                    log(\"ERROR\", \"Error de conexion: {e}\")\n                    time.sleep(2 ** attempt)\n                \n            if not success:\n                log(\"ERROR\", \"Se extendieron los intentos para el dia: {current_date.date()}\")\n                break\n\n            data = response.json()\n            batch = data.get('QueryResponse', {}).get('Customer', [])\n\n            if not batch:\n                break\n\n            daily_pages += 1\n            daily_rows += len(batch)\n\n            ingestion_time = datetime.now(pytz.utc)\n\n            for item in batch:\n                record = {\n                    'id': item['Id'], \n                    'payload': item,  \n                    'ingested_at_utc': ingestion_time,\n                    'extract_window_start_utc': start_fmt,\n                    'extract_window_end_utc': end_fmt,\n                    'page_number': (start_position // max_results) + 1,\n                    'page_size': len(batch),\n                    'request_payload': query \n                }\n                all_records.append(record)\n\n            start_position += max_results\n            if len(batch) < max_results:\n                break\n                \n\n        chunk_duration = time.time() - chunk_start_time\n        log(\"METRIC\", f\"D\u00eda: {current_date.date()} | Filas: {daily_rows} | Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        if daily_rows > 0:\n            log(\"REPORTE\", f\"Reporte tramo {current_date.date()}:\")\n            log(\"REPORTE\", f\"Paginas: {daily_pages}\")\n            log(\"REPORTE\", f\"Filas: {daily_rows}\")\n            log(\"REPORTE\", f\"Duracion: {chunk_duration:.2f}\")\n        else:\n            log(\"VOLUMETRY_WARN\", f\"0 registros para {current_date.date()}\")\n\n        current_date = next_date\n    \n    log(\"DONE\", f'Total: {len(all_records)} registros')\n    return pd.DataFrame(all_records)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/default_repo/data_loaders/load_qdo_data.py", "language": "python", "type": "data_loader", "uuid": "load_qdo_data"}, "/home/src/default_repo/data_loaders/load_items_qbo.py:data_loader:python:home/src/default repo/data loaders/load items qbo": {"content": "import io\nimport pandas as pd\nimport requests\nimport base64\nimport time\nfrom datetime import timedelta, datetime\nimport pytz\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\ndef get_access_token():\n    log(\"AUTH\", \"Solicitando nuevo token...\")\n    client_id = get_secret_value('QBO_CLIENT_ID')\n    client_secret = get_secret_value('QBO_CLIENT_SECRET')\n    refresh_token = get_secret_value('QBO_REFRESH_TOKEN')\n\n    auth_str = f'{client_id}:{client_secret}'\n    b64_auth = base64.b64encode(auth_str.encode()).decode()\n\n    url = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n    headers = {\n        'Authorization' : f'Basic {b64_auth}',\n        'Content-Type' : 'application/x-www-form-urlencoded',\n        'Accept' : 'application/json'\n    }\n\n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data)\n        if response.status_code != 200:\n            log(\"AUTH_ERROR\", f\"Fallo al renovar: {response.text}\")\n            raise Exception(f\"Error while renewing token: {response.text}\")\n        return response.json().get('access_token')\n    except Exception as e:\n        log(\"AUTH_ERROR\", str(e))\n        raise e\n    \n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    now = datetime.now(pytz.utc)\n    default_start = (now - timedelta(days=2)).strftime('%Y-%m-%d')\n    default_end = now.strftime('%Y-%m-%d')\n\n    start_date_str = kwargs.get('fecha_inicio', default_start)\n    end_date_str = kwargs.get('fecha_fin', default_end)\n\n    log(\"INIT\", f\"Iniciando extracci\u00f3n desde {start_date_str} hasta {end_date_str}\")\n\n    access_token = get_access_token()\n    realm_id = get_secret_value('QBO_REALM_ID')\n\n    #query = \"SELECT * FROM Customer STARTPOSITION 1 MAXRESULTS 1\"\n    base_url = f\"https://sandbox-quickbooks.api.intuit.com/v3/company/{realm_id}/query?minorversion=65\"\n\n    headers = {\n        'Authorization' : f'Bearer {access_token}',\n        'Accept' : 'application/json',\n        'Content-Type' : 'application/json'\n    }\n\n    all_records = []\n\n    start_dt = pd.to_datetime(start_date_str).replace(tzinfo=pytz.UTC)\n    end_dt = pd.to_datetime(end_date_str).replace(tzinfo=pytz.UTC)\n\n    print(f\"Rango de datos: {start_dt} hasta {end_dt}\")\n\n    current_date = start_dt\n\n    while current_date <= end_dt:\n\n        chunk_start_time = time.time()\n        daily_pages = 0\n        daily_rows = 0\n\n        next_date = current_date + timedelta(days=1)\n\n        start_fmt = current_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n        end_fmt = next_date.strftime('%Y-%m-%dT%H:%M:%S-00:00')\n\n        start_position = 1\n        max_results = 1000\n\n        while True:\n            query = (\n                f\"SELECT * FROM Item \"\n                f\"WHERE MetaData.LastUpdatedTime >= '{start_fmt}' \"\n                f\"AND MetaData.LastUpdatedTime <= '{end_fmt}' \"\n                f\"STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n            )\n\n            max_retries = 5\n            success = False\n\n            for attempt in range(max_retries):\n                try:\n                    response = requests.get(base_url, headers=headers, params={'query':query})\n                    if response.status_code == 401:\n                        log(\"AUTH\", 'Token expirado, obteniendo uno nuevo...')\n                        access_token = get_access_token()\n                        headers['Authorization'] = f\"Bearer {access_token}\"\n                        continue\n\n                    if response.status_code == 429:\n                        wait_time = 2 ** attempt \n                        log(\"WARN\", f\"Rate Limit (429). Esperando {wait_time}s antes del reintento {attempt+1}/{max_retries}...\")\n                        time.sleep(wait_time)\n                        continue\n\n                    if response.status_code >= 500:\n                        wait_time = 2 ** attempt\n                        log(\"SERVER\", f\"Error Servidor ({response.status_code}). Esperando {wait_time}s...\")\n                        time.sleep(wait_time)\n                        continue\n                    \n                    if response.status_code != 200:\n                        log(\"ERROR\", f\"Error al obtener datos del dia: {current_date}: {response.text}\")\n                        break\n                    \n                    success = True\n                    break\n\n                except Exception as e:\n                    log(\"ERROR\", \"Error de conexion: {e}\")\n                    time.sleep(2 ** attempt)\n                \n            if not success:\n                log(\"ERROR\", \"Se extendieron los intentos para el dia: {current_date.date()}\")\n                break\n\n            data = response.json()\n            batch = data.get('QueryResponse', {}).get('Item', [])\n\n            if not batch:\n                break\n\n            daily_pages += 1\n            daily_rows += len(batch)\n\n            ingestion_time = datetime.now(pytz.utc)\n\n            for item in batch:\n                record = {\n                    'id': item['Id'], \n                    'payload': item,  \n                    'ingested_at_utc': ingestion_time,\n                    'extract_window_start_utc': start_fmt,\n                    'extract_window_end_utc': end_fmt,\n                    'page_number': (start_position // max_results) + 1,\n                    'page_size': len(batch),\n                    'request_payload': query \n                }\n                all_records.append(record)\n\n            start_position += max_results\n            if len(batch) < max_results:\n                break\n                \n\n        chunk_duration = time.time() - chunk_start_time\n        log(\"METRIC\", f\"D\u00eda: {current_date.date()} | Filas: {daily_rows} | Duraci\u00f3n: {chunk_duration:.2f}s\")\n\n        if daily_rows > 0:\n            log(\"REPORTE\", f\"Reporte tramo {current_date.date()}:\")\n            log(\"REPORTE\", f\"Paginas: {daily_pages}\")\n            log(\"REPORTE\", f\"Filas: {daily_rows}\")\n            log(\"REPORTE\", f\"Duracion: {chunk_duration:.2f}\")\n        else:\n            log(\"VOLUMETRY_WARN\", f\"0 registros para {current_date.date()}\")\n\n        current_date = next_date\n    \n    log(\"DONE\", f'Total: {len(all_records)} registros')\n    return pd.DataFrame(all_records)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/default_repo/data_loaders/load_items_qbo.py", "language": "python", "type": "data_loader", "uuid": "load_items_qbo"}, "/home/src/default_repo/data_exporters/export_customers_raw.py:data_exporter:python:home/src/default repo/data exporters/export customers raw": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\nimport psycopg2\nimport json\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    if data is None or data.empty:\n        log(\"WARN\", \"No llegaron datos para exportar. Pipeline finalizado.\")\n        return\n\n    try:\n        db_host = get_secret_value('POSTGREST_HOST')\n        db_name = get_secret_value('POSTGRES_DB')\n        db_user = get_secret_value('POSTGRES_USER')\n        db_pass = get_secret_value('POSTGRES_PASSWORD')\n    except Exception as e:\n        log(\"CONFIG_ERROR\", f\"Error en credenciales, {e}\")\n        return\n\n    log(\"DATA_PROCESS\", f\"Guardando {len(data)} datos\")\n\n    conn = psycopg2.connect(\n        host=db_host,\n        database=db_name,\n        user=db_user,\n        password=db_pass\n    )\n    cursor = conn.cursor()\n\n    insert_query = \"\"\"\n    INSERT INTO raw.qb_customers_backfill (\n        id, payload, ingested_at_utc, extract_window_start_utc, \n        extract_window_end_utc, page_number, page_size, request_payload\n    ) \n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (id) DO UPDATE SET\n        payload = EXCLUDED.payload,\n        ingested_at_utc = EXCLUDED.ingested_at_utc,\n        extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n        extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n        page_number = EXCLUDED.page_number,\n        page_size = EXCLUDED.page_size,\n        request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    success_count = 0\n    error_count = 0\n\n    try:\n        for _, row in data.iterrows():\n\n            try:\n                cursor.execute(insert_query, (\n                    str(row['id']),\n                    json.dumps(row['payload']),\n                    row['ingested_at_utc'],\n                    row['extract_window_start_utc'],\n                    row['extract_window_end_utc'],\n                    row['page_number'],\n                    row['page_size'],\n                    json.dumps(row['request_payload'])\n                ))\n                success_count += 1\n            except Exception as e:\n                error_count += 1\n                log(\"ROW_ERROR\", f\"Fallo en ID {row.get('id', 'unknown')}: {e}\")\n                conn.rollback()\n                raise e\n\n        \n        conn.commit()\n        log(\"METRIC\", \"Datos guardados correctamente\")\n    except Exception as e:\n        conn.rollback()\n        log(\"CRITICAL\", f\"Error al guardar datos: {e}\")\n        raise e\n    finally:\n        cursor.close()\n        conn.close()", "file_path": "/home/src/default_repo/data_exporters/export_customers_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_customers_raw"}, "/home/src/default_repo/data_exporters/export_items_raw.py:data_exporter:python:home/src/default repo/data exporters/export items raw": {"content": "from mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom datetime import datetime\nimport psycopg2\nimport json\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\ndef log(phase, message):\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] [{phase}] {message}\")\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n\n    if data is None or data.empty:\n        log(\"WARN\", \"No llegaron datos para exportar. Pipeline finalizado.\")\n        return\n\n    try:\n        db_host = get_secret_value('POSTGREST_HOST')\n        db_name = get_secret_value('POSTGRES_DB')\n        db_user = get_secret_value('POSTGRES_USER')\n        db_pass = get_secret_value('POSTGRES_PASSWORD')\n    except Exception as e:\n        log(\"CONFIG_ERROR\", f\"Error en credenciales, {e}\")\n        return\n\n    log(\"DATA_PROCESS\", f\"Guardando {len(data)} datos\")\n\n    conn = psycopg2.connect(\n        host=db_host,\n        database=db_name,\n        user=db_user,\n        password=db_pass\n    )\n    cursor = conn.cursor()\n\n    insert_query = \"\"\"\n    INSERT INTO raw.qb_items_backfill (\n        id, payload, ingested_at_utc, extract_window_start_utc, \n        extract_window_end_utc, page_number, page_size, request_payload\n    ) \n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n    ON CONFLICT (id) DO UPDATE SET\n        payload = EXCLUDED.payload,\n        ingested_at_utc = EXCLUDED.ingested_at_utc,\n        extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n        extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n        page_number = EXCLUDED.page_number,\n        page_size = EXCLUDED.page_size,\n        request_payload = EXCLUDED.request_payload;\n    \"\"\"\n\n    success_count = 0\n    error_count = 0\n\n    try:\n        for _, row in data.iterrows():\n\n            try:\n                cursor.execute(insert_query, (\n                    str(row['id']),\n                    json.dumps(row['payload']),\n                    row['ingested_at_utc'],\n                    row['extract_window_start_utc'],\n                    row['extract_window_end_utc'],\n                    row['page_number'],\n                    row['page_size'],\n                    json.dumps(row['request_payload'])\n                ))\n                success_count += 1\n            except Exception as e:\n                error_count += 1\n                log(\"ROW_ERROR\", f\"Fallo en ID {row.get('id', 'unknown')}: {e}\")\n                conn.rollback()\n                raise e\n\n        \n        conn.commit()\n        log(\"METRIC\", \"Datos guardados correctamente\")\n    except Exception as e:\n        conn.rollback()\n        log(\"CRITICAL\", f\"Error al guardar datos: {e}\")\n        raise e\n    finally:\n        cursor.close()\n        conn.close()", "file_path": "/home/src/default_repo/data_exporters/export_items_raw.py", "language": "python", "type": "data_exporter", "uuid": "export_items_raw"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}